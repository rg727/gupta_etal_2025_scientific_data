
# Gupta-etal_2025_scientific_data

Rohini S. Gupta <sup>1\*</sup>, Sungwook Wi <sup>1\*</sup> Scott Steinschneider<sup>1</sup>

<sup>1 </sup> Department of Biological and Environmental Engineering, Cornell University, Ithaca, NY

\* corresponding author:  rg727@cornell.edu

## Abstract
High-quality regional streamflow datasets are necessary to support local water resources planning and management. However, observed streamflow records are often limited by the availability of surface water gauges, which frequently go in and out of service over time. Long-term reconstruction products that fill in missing data are not consistently available. This challenge is prevalent across the United States, including in the Great Lakes region, which contains 20% of the world’s freshwater and serves as a critical resource for both the United States and Canada. In the Great Lakes, there is also a specific interest in estimating aggregate runoff into the lakes to better understand the regional water balance and lake level variability. Existing aggregate runoff data products are typically derived from runoff area ratios or process-based models, but these approaches are hindered by model parameter uncertainty and a limited ability to capture the vast spatial heterogeneity of the basin. Furthermore, of the products that exist, most do not start until 1980.
In this work, we develop a new, historical reconstruction of daily streamflow at over 650 gauged locations throughout the Great Lakes basin using a novel regional Long Short-Term Memory (LSTM) model that integrates local climate data, physical catchment characteristics, and runoff observations from nearby gauged sites. We also estimate monthly runoff into the lakes for the period of 1950-2013. The daily reconstruction product will equip water managers with information to understand emerging hydroclimate trends and provide a basis to support local water resources analyses. The aggregate runoff product shows strong potential for improving estimates of monthly lake-wide runoff, which can ultimately help resolve the complete water balance of the Great Lakes and provide critical context for lake level shifts under a changing climate.

## Journal reference
Gupta, R. S., Wi, S., Steinschneider, S. (In Preparation). Machine Learning-Based Reconstructions of Historical Daily and Monthly Runoff for the Laurentian Great Lakes Region. Nature Scientific Data

## Code reference

All code to reproduce the results/figures is found in this repository or is linked below. 

## Data reference

### Input data and Output Data
Gupta, R.S., Steinschneider, S., Reed, P.M. (2024). Gupta-et-al_2024_EarthsFuture (Version v1) [Data set]. MSD-LIVE Data Repository. https://doi.org/10.57931/2458138


## Contributing modeling software
| Model | Version | Repository Link | DOI |
|-------|---------|-----------------|-----|
| CALFEWS- Historical | V 1.0 | https://github.com/hbz5000/CALFEWS |10.5281/zenodo.4091708|


## Reproduce my experiment

There are two components to this study: (1) produce input files for CALFEWS and (2) run CALFEWS under historical, paleo, and climate change scenarios and synthesize the output. 

### Produce Input Files
Full natural flow and snow that serves as an input into CALFEWS can be found in the “hydroclimate” folder in the MSD-Live repository. The streamflow is sorted into CDEC, Paleo, and 4T, 1CC (the selected climate change scenario). 

Use the scripts found in the `create_inputs` directory.

| Script Name | Description | How to Run |
| --- | --- | --- |
| `make_subfolder.sh`| This bash script creates a folder structure to store the input data for CALFEWS. It creates a folder for the baseline (0T, 0CC) and climate change scenarios, all 50 ensemble members, and each 30-year section| `sbatch make_subfolder.sh` |
| `splitting_script.R`, `splitting_script_CC.R` | These scripts take the streamflow and snow and partition them into 30-year periods for the baseline and climate change scenarios. For this study, we just use period "6" which corresponds to 1550-1580, though the user can create inputs for any section| `sbatch split_datasets.sh` |
| `initialize_params.sh` | This script creates the `base_inflows.json` file needed to initialize CALFEWS. It adjusts the file to take in the location of the 30-year dataset | `python3 step_two.py -o /path/to/my/outputdir` |


### Running Historical CALFEWS 

Baseline values using modeled SAC-SMA historical flows are generated by running CALFEWS with a few changes that are found in `./run_CALFEWS`. 

1. Clone the CALFEWS repository from: `https://github.com/hbz5000/CALFEWS`
2. Swap out the `./runtime_parameters.ini` with the new one in `./run_baseline_CALFEWS`. This sets the input source to be the historical sacsma_dataset
3. Add in  `sacsma_baseline.csv` and `sacsma_data.csv` to `./calfews_src/data/input/`
4. Swap out the `base_inflows.json` file with the one in `./run_baseline_CALFEWS`
5. Swap out `main_cy.pyx` with the one in `./run_baseline_CALFEWS`

Follow the directions in the CALFEWS readme to run this version of CALFEWS. The output will be a `results.hdf5` file.    

### Running Paleo or Climate Change- Forced CALFEWS 

1. Running CALFEWS under paleo and climate change scenarios uses the CALFEWS model created above. However, we can run CALFEWS in batch across multiple ensembles of inputs that we generated above and that we will store in respective folders. 

| Script Name | Description | How to Run |
| --- | --- | --- |
| `run_calfews.py`| This bash script runs CALFEWS for a specific paleo or climate change scenario and ensemble member. A `results.hdf` file will be created in respective output folders| `sbatch run_calfews.sh` |

### Creating Intermediary Scripts 

Once the results files are created, a variety of scripts are used to aggregate the results into intermediary products to be used in the plotting scripts. 

Use the scripts found in the `create_intermediary_products` directory.

| Script Name | Description | How to Run |
| --- | --- | --- |
| `make_quantile_products.py`| This script creates the baseline and climate change products for Figure 5| `python make_quantile_products.py` |
| `make_reservoir_pkls.py`| This script creates reservoir storage files for Figure 6| `python make_reservoir_pkls.py` |
| `contract_deliveries.py`| This script creates aggregate datasets for contract deliveries for multiple figures| `contract_deliveries.py` |
| `delta_cummulative_risk.py`| This script creates aggregate datasets the delta cummulative risk lineplot in Figure 10| `delta_cummulative_risk.py` |

## Reproduce my figures
Use the scripts found in the `figures` directory to reproduce the figures used in this publication.

| Figure Number(s) | Script Name | Description | How to Run |
| --- | --- | --- | --- |
| 4 | `makeFigure4.py` | Make Figure 4 (Hydrology of the Megadrought) | `python makeFigure4.py` |
| 5 | `makeFigure5.py` | Make Figure 5 (Reservoir Storage During the Megadrought) | `python makeFigure5.py` |
| 6 | `makeFigure6.py` | Make Figure 6 (Climate Change Effects on Reservoir Storage and Deliveries) | `python makeFigure6.py` |
| 7 | `makeFigure7.py` | Make Figure 7 (Reservoir Inflows and Full Natural Flows) | `python makeFigure7.py` |
| 8 | `makeFigure8.py` | Make Figure 8 (Kernel Density Plots-Sac/San Joaquin) | `python makeFigure8.py` |
| 9 | `makeFigure9.py` | Make Figure 9 (Duration-Severity Plots) | `python makeFigure9.py` |
| 10 | `makeFigure10.py` | Make Figure 10 (Delta Metrics) | `python makeFigure10.py` |
| 11 | `makeFigure11.py` | Make Figure 11 (Kernel Density Plots-Tulare Basin) | `python makeFigure11.py` |
| 12 | `makeFigure12.py` | Make Figure 12 (Kernel Density Plots-Groundwater Bank Accounts) | `python makeFigure12.py` |



